{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Playing with numpy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3JSKwfcjBGQYoPpkvbV93",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushantp58/Neural-Nets-Repo/blob/master/Playing_with_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ1XXcaE2Ts8"
      },
      "source": [
        "# building basic functions with numpy\n",
        "#1 . sigmoid function using np.exp(), \n",
        "#sigmoid(x) also known as logistic function which is more of a non linear function used not in ML but also in DL\n",
        "import math\n",
        "def basic_sigmoid(x):\n",
        "  \"\"\"\n",
        "  compute sigmoid of x.\n",
        "\n",
        "  Argument:\n",
        "  x -- A scalar\n",
        "\n",
        "  Return:\n",
        "  s -- Sigmoid(x)\n",
        "  \"\"\"\n",
        "  s = 1/(1 + math.exp(-x))\n",
        "  return s"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgDTYg9G2Wie",
        "outputId": "1a4faa82-0e87-49e0-b059-57c400d92677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "basic_sigmoid(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9525741268224334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r98lmTnzHEW4",
        "outputId": "cf156ad6-724b-4367-e006-002ce0f9e610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "## to explain why we don't use math.exp(x) in dl is because the inputs of the functions are real numbers \n",
        "## and here we use matrices and vectos which is why numpy is essential\n",
        "x = [1,2,3]\n",
        "# basic_sigmoid(x) -- uncomment to see the demo\n",
        "import numpy as np\n",
        "x = np.array([1,2,3])\n",
        "print(np.exp(x))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.71828183  7.3890561  20.08553692]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIpBmNOeHr5X"
      },
      "source": [
        "def sigmoid(x):\n",
        "  \"\"\"\n",
        "  compute sigmoid of x.\n",
        "\n",
        "  Argument:\n",
        "  x -- A scalar\n",
        "\n",
        "  Return:\n",
        "  s -- Sigmoid(x)\n",
        "  \"\"\"\n",
        "  s = 1 / (1 + np.exp(-x))\n",
        "  return s\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXf-klpxIW42",
        "outputId": "208186cf-51c0-4038-e467-fd135a303d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = np.array([1, 2, 3])\n",
        "sigmoid(x)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.73105858, 0.88079708, 0.95257413])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr-bIUg1Ikiq"
      },
      "source": [
        "## Sigmoid gradient\n",
        "## base function for further calculation of backpropagation\n",
        "## sigmoid derivative(x) = sigma'(x)(1-sigma(x))\n",
        "def sigmoid_Derivative(x):\n",
        "  \"\"\"\n",
        "  Compute The gradient or slope of the derivative of the sigmoid function wrt to its input x.\n",
        "  you can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
        "\n",
        "  Arguments:\n",
        "  x -- A Scalar or numpy array\n",
        "\n",
        "  Return\n",
        "  ds -- Computed gradient\n",
        "  \"\"\"\n",
        "  s = sigmoid(x)\n",
        "  ds = s * (1-s)\n",
        "  return ds"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3n0tv76KCDY",
        "outputId": "87406eef-db34-4329-824e-e73d107220e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = np.array([1, 2, 3])\n",
        "sigmoid_Derivative(x)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.19661193, 0.10499359, 0.04517666])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W68_ocYAKHHr"
      },
      "source": [
        "## Reshaping Arrays\n",
        "## Always use in conjunction (shape,reshape) shape for getting ddimension of matix and reshape for reshaping x into some other dimension\n",
        "def image2vector(image):\n",
        "  \"\"\"\n",
        "  Argument:\n",
        "   image -- a numpy array of shape (length,height,depth)\n",
        "\n",
        "  Returns:\n",
        "   v -- a vecotr of shape (lhb,1)\n",
        "  \"\"\"\n",
        "  v = image\n",
        "  v = v.reshape((v.shape[0]*v.shape[1],v.shape[2]))\n",
        "  return v"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzf1pJfPMQ6r",
        "outputId": "7c649598-2ed5-422b-ffaa-169d896619b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
        "image = np.array([[[ 0.67826139,  0.29380381],\n",
        "        [ 0.90714982,  0.52835647],\n",
        "        [ 0.4215251 ,  0.45017551]],\n",
        "\n",
        "       [[ 0.92814219,  0.96677647],\n",
        "        [ 0.85304703,  0.52351845],\n",
        "        [ 0.19981397,  0.27417313]],\n",
        "\n",
        "       [[ 0.60659855,  0.00533165],\n",
        "        [ 0.10820313,  0.49978937],\n",
        "        [ 0.34144279,  0.94630077]]])\n",
        "\n",
        "print (\"image2vector(image) = \" + str(image2vector(image)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image2vector(image) = [[0.67826139 0.29380381]\n",
            " [0.90714982 0.52835647]\n",
            " [0.4215251  0.45017551]\n",
            " [0.92814219 0.96677647]\n",
            " [0.85304703 0.52351845]\n",
            " [0.19981397 0.27417313]\n",
            " [0.60659855 0.00533165]\n",
            " [0.10820313 0.49978937]\n",
            " [0.34144279 0.94630077]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUuxZNEsMZ2y"
      },
      "source": [
        "### Normalizing rows is an important  technique used in ml and dl for normalization of data\n",
        "### it often leads to a better performance coz gradient descent converges faster after normalization\n",
        "### Dividing each vector by it's norm\n",
        "\n",
        "def normalizeRows(x):\n",
        "  \"\"\"\n",
        "  Implementation of a function which normalizes row of a matrix x to have unit length\n",
        "\n",
        "  Argument:\n",
        "  x -- A numpy matrix of shape (n,m)\n",
        "\n",
        "  Returns:\n",
        "  x -- The normalized row numpy matrix\n",
        "\n",
        " \"\"\"\n",
        "  x_norm = np.linalg.norm(x,axis = 1,keepdims=True)\n",
        "  x = x / x_norm\n",
        "  return x"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Euff-89OYLS",
        "outputId": "674ca2dc-e273-4685-883d-a882fee6fbd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x = np.array([\n",
        "    [0, 3, 4],\n",
        "    [1, 6, 4]])\n",
        "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
            " [0.13736056 0.82416338 0.54944226]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ctVgrxuOk7D"
      },
      "source": [
        "### Broadcasting function - which is useful for performing mathematical operations b/w arrays of different shapes\n",
        "### softmax function using numpy which is a normalizing function when algorithm needs to classify two or more classes\n",
        "def softmax(x):\n",
        "  \"\"\"\n",
        "  Calculates the softmax for each row of the input row vector x .\n",
        "\n",
        "  Argument:\n",
        "  x  -- A numpy matrix of shape(m,n)\n",
        "\n",
        "  Returns:\n",
        "  s -- A numpy matric equal to softmax \n",
        "  \n",
        "  \"\"\"\n",
        "  x_exp = np.exp(x)\n",
        "  x_sum = np.sum(x_exp,axis = 1,keepdims = True)\n",
        "  s = x_exp / x_sum\n",
        "  return s"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuoWdRfjUJ68",
        "outputId": "fd0da485-679a-41d6-85b2-425211df7e9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "x = np.array([\n",
        "    [9, 2, 5, 0, 0],\n",
        "    [7, 5, 0, 0 ,0]])\n",
        "print(\"softmax(x) = \" + str(softmax(x)))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
            "  1.21052389e-04]\n",
            " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
            "  8.01252314e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVKou5y7ULxM",
        "outputId": "f0b82edc-1849-4e94-8182-80be809c69b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "## Vectorization - Demo of difference between Classic and vectorized computations\n",
        "import time\n",
        "\n",
        "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
        "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
        "\n",
        "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "dot = 0\n",
        "for i in range(len(x1)):\n",
        "    dot+= x1[i]*x2[i]\n",
        "toc = time.process_time()\n",
        "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
        "for i in range(len(x1)):\n",
        "    for j in range(len(x2)):\n",
        "        outer[i,j] = x1[i]*x2[j]\n",
        "toc = time.process_time()\n",
        "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "mul = np.zeros(len(x1))\n",
        "for i in range(len(x1)):\n",
        "    mul[i] = x1[i]*x2[i]\n",
        "toc = time.process_time()\n",
        "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
        "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
        "tic = time.process_time()\n",
        "gdot = np.zeros(W.shape[0])\n",
        "for i in range(W.shape[0]):\n",
        "    for j in range(len(x1)):\n",
        "        gdot[i] += W[i,j]*x1[j]\n",
        "toc = time.process_time()\n",
        "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dot = 278\n",
            " ----- Computation time = 0.07428400000009105ms\n",
            "outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
            " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
            " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
            " ----- Computation time = 0.25562600000128555ms\n",
            "elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
            " ----- Computation time = 0.11140600000025813ms\n",
            "gdot = [24.33371835 22.39790943 29.63118818]\n",
            " ----- Computation time = 0.21534599999917248ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW0xY2-BXmB9",
        "outputId": "606d0d7a-b415-4391-bc20-52e08c2ffea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
        "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
        "\n",
        "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
        "tic = time.process_time()\n",
        "dot = np.dot(x1,x2)\n",
        "toc = time.process_time()\n",
        "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### VECTORIZED OUTER PRODUCT ###\n",
        "tic = time.process_time()\n",
        "outer = np.outer(x1,x2)\n",
        "toc = time.process_time()\n",
        "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
        "tic = time.process_time()\n",
        "mul = np.multiply(x1,x2)\n",
        "toc = time.process_time()\n",
        "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### VECTORIZED GENERAL DOT PRODUCT ###\n",
        "tic = time.process_time()\n",
        "dot = np.dot(W,x1)\n",
        "toc = time.process_time()\n",
        "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dot = 278\n",
            " ----- Computation time = 0.09580900000116799ms\n",
            "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
            " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
            " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            " ----- Computation time = 0.09442600000042489ms\n",
            "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
            " ----- Computation time = 0.10237899999943068ms\n",
            "gdot = [24.33371835 22.39790943 29.63118818]\n",
            " ----- Computation time = 0.1363639999993893ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY9T9S2OXoz0"
      },
      "source": [
        "## the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, the differences in running time become even bigger. \n",
        "## Implementations of L1 and L2 functions\n",
        "## L1 loss  - used for evaluation of the performance of the model , bigger the loss , more efficient are your predictions of your values from your true values\n",
        "## convex optimizations like GD are used for training ur model and minimizing cost\n",
        "\n",
        "def L1(yhat,y):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  yhat - predicted labels of size m\n",
        "  y -- vector of size m\n",
        "\n",
        "  Returns:\n",
        "  loss - the value of L1 loss functions defined\n",
        "\n",
        "  \"\"\"\n",
        "  loss = np.sum(abs(yhat - y))\n",
        "  return loss"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhMgCZ7GX3up",
        "outputId": "bbd9aa76-6d16-46e5-de10-1bf6f6a114ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "print(\"L1 = \" + str(L1(yhat,y)))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L1 = 1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FdCptKDZSF5"
      },
      "source": [
        "## next is l2 loss which is second norm of l1\n",
        "def L2(yhat,y):\n",
        "  loss =  np.dot(y - yhat,y - yhat)\n",
        "  return loss"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu11AtGzZmA4",
        "outputId": "7bbdf41c-7d29-4f58-d981-2b271ea89d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "print(\"L2 = \" + str(L2(yhat,y)))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 = 0.43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGDVveFVoBlg"
      },
      "source": [
        " What to remember:\n",
        "\n",
        "\n",
        "    np.exp(x) works for any np.array x and applies the exponential function to every coordinate\n",
        "    the sigmoid function and its gradient\n",
        "    image2vector is commonly used in deep learning\n",
        "    np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs.\n",
        "    numpy has efficient built-in functions\n",
        "    broadcasting is extremely useful\n",
        "\n",
        "    Vectorization is very important in deep learning. It provides computational efficiency and clarity.\n",
        "    reviewed the L1 and L2 loss.\n",
        "    familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc...\n"
      ]
    }
  ]
}